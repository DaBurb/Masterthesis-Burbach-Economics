{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59a74e55-0d91-4979-8b0c-ac4f25a5ee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2010_figaro_64.csv...\n",
      "Processed and saved tables for the year 2010 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2010\n",
      "Processing 2011_figaro_64.csv...\n",
      "Processed and saved tables for the year 2011 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2011\n",
      "Processing 2012_figaro_64.csv...\n",
      "Processed and saved tables for the year 2012 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2012\n",
      "Processing 2013_figaro_64.csv...\n",
      "Processed and saved tables for the year 2013 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2013\n",
      "Processing 2014_figaro_64.csv...\n",
      "Processed and saved tables for the year 2014 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2014\n",
      "Processing 2015_figaro_64.csv...\n",
      "Processed and saved tables for the year 2015 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2015\n",
      "Processing 2016_figaro_64.csv...\n",
      "Processed and saved tables for the year 2016 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2016\n",
      "Processing 2017_figaro_64.csv...\n",
      "Processed and saved tables for the year 2017 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2017\n",
      "Processing 2018_figaro_64.csv...\n",
      "Processed and saved tables for the year 2018 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2018\n",
      "Processing 2019_figaro_64.csv...\n",
      "Processed and saved tables for the year 2019 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2019\n",
      "Processing 2020_figaro_64.csv...\n",
      "Processed and saved tables for the year 2020 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2020\n",
      "Processing 2021_figaro_64.csv...\n",
      "Processed and saved tables for the year 2021 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2021\n",
      "Processing 2022_figaro_64.csv...\n",
      "Processed and saved tables for the year 2022 in directory: C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables\\2022\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os as os\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Sector Mapping for Standardization\n",
    "# ---------------------------------------------------\n",
    "\n",
    "sector_mapping = {\n",
    "    \"C10T12\": \"C10-C12\",\n",
    "    \"C13T15\": \"C13-C15\",\n",
    "    \"C31_32\": \"C31_C32\",\n",
    "    \"E37T39\": \"E37-E39\",\n",
    "    \"J59_60\": \"J59_J60\",\n",
    "    \"J62_63\": \"J62_J63\",\n",
    "    \"M69_70\": \"M69_M70\",\n",
    "    \"M74_75\": \"M74_M75\",\n",
    "    \"L\":\"L68\"\n",
    "}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Apply Sector Mapping\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def apply_sector_mapping(df):\n",
    "    \"\"\"\n",
    "    Replace sector names in both rows (index) and columns based on the mapping,\n",
    "    accounting for leading underscores.\n",
    "    \"\"\"\n",
    "    # Adjust sector mapping to handle leading underscores\n",
    "    sector_pattern = {rf'_{re.escape(k)}\\b': f\"_{v}\" for k, v in sector_mapping.items()}\n",
    "\n",
    "    # Apply mapping to rows (index)\n",
    "    df.index = df.index.to_series().replace(sector_pattern, regex=True)\n",
    "\n",
    "    # Apply mapping to columns\n",
    "    df.columns = df.columns.to_series().replace(sector_pattern, regex=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Helper Functions for Prefixing Rows and Columns\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def prefix_rows(df):\n",
    "    df.index = ['op_' + str(row) for row in df.index]\n",
    "    return df\n",
    "\n",
    "def prefix_columns(df):\n",
    "    df.columns = ['ip_' + str(col) for col in df.columns]\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Function to Extract Country Codes from Columns\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def extract_country_codes(df):\n",
    "    country_codes = set()\n",
    "    for col in df.columns:\n",
    "        match = re.match(r'ip_([A-Z]{2,5})_', col)\n",
    "        if match:\n",
    "            country_codes.add(match.group(1))\n",
    "\n",
    "    if 'FIGW1' not in country_codes and any(col.startswith('ip_FIGW1') for col in df.columns):\n",
    "        country_codes.add('FIGW1')\n",
    "\n",
    "    return list(country_codes)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Column and Row Aggregation Functions\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def aggregate_columns(df, country_prefix):\n",
    "    columns_to_aggregate = {\n",
    "        'N': [f'ip_{country_prefix}_N77', f'ip_{country_prefix}_N78', f'ip_{country_prefix}_N79', f'ip_{country_prefix}_N80T82'],\n",
    "        'Q': [f'ip_{country_prefix}_Q86', f'ip_{country_prefix}_Q87_88'],\n",
    "        'R_S': [f'ip_{country_prefix}_R90T92', f'ip_{country_prefix}_R93', f'ip_{country_prefix}_S94', f'ip_{country_prefix}_S95', f'ip_{country_prefix}_S96']\n",
    "    }\n",
    "    new_columns = {}\n",
    "    for new_col, old_cols in columns_to_aggregate.items():\n",
    "        available_cols = [col for col in old_cols if col in df.columns]\n",
    "        if available_cols:\n",
    "            new_columns[f'ip_{country_prefix}_{new_col}'] = df[available_cols].sum(axis=1)\n",
    "\n",
    "    return pd.DataFrame(new_columns)\n",
    "\n",
    "def aggregate_rows(df, country_prefix):\n",
    "    rows_to_aggregate = {\n",
    "        'N': [f'op_{country_prefix}_N77', f'op_{country_prefix}_N78', f'op_{country_prefix}_N79', f'op_{country_prefix}_N80T82'],\n",
    "        'Q': [f'op_{country_prefix}_Q86', f'op_{country_prefix}_Q87_88'],\n",
    "        'R_S': [f'op_{country_prefix}_R90T92', f'op_{country_prefix}_R93', f'op_{country_prefix}_S94', f'op_{country_prefix}_S95', f'op_{country_prefix}_S96']\n",
    "    }\n",
    "    new_rows = {}\n",
    "    for new_row, old_rows in rows_to_aggregate.items():\n",
    "        available_rows = [row for row in old_rows if row in df.index]\n",
    "        if available_rows:\n",
    "            new_rows[f'op_{country_prefix}_{new_row}'] = df.loc[available_rows].sum()\n",
    "\n",
    "    return pd.DataFrame(new_rows).T\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Functions to Remove Old Rows and Columns\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def remove_old_columns(df, country_prefix):\n",
    "    columns_to_remove = [\n",
    "        f'ip_{country_prefix}_N77', f'ip_{country_prefix}_N78', f'ip_{country_prefix}_N79', f'ip_{country_prefix}_N80T82',\n",
    "        f'ip_{country_prefix}_Q86', f'ip_{country_prefix}_Q87_88',\n",
    "        f'ip_{country_prefix}_R90T92', f'ip_{country_prefix}_R93', f'ip_{country_prefix}_S94', f'ip_{country_prefix}_S95', f'ip_{country_prefix}_S96'\n",
    "    ]\n",
    "    return df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "def remove_old_rows(df, country_prefix):\n",
    "    rows_to_remove = [\n",
    "        f'op_{country_prefix}_N77', f'op_{country_prefix}_N78', f'op_{country_prefix}_N79', f'op_{country_prefix}_N80T82',\n",
    "        f'op_{country_prefix}_Q86', f'op_{country_prefix}_Q87_88',\n",
    "        f'op_{country_prefix}_R90T92', f'op_{country_prefix}_R93', f'op_{country_prefix}_S94', f'op_{country_prefix}_S95', f'op_{country_prefix}_S96'\n",
    "    ]\n",
    "    return df.drop(index=[row for row in rows_to_remove if row in df.index])\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Insertion Functions for Aggregated Rows and Columns\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def insert_aggregated_columns(df, new_columns, country_prefix):\n",
    "    for col_name in new_columns.columns:\n",
    "        if col_name.endswith('_N'):\n",
    "            insert_after = f'ip_{country_prefix}_M74_M75'\n",
    "        elif col_name.endswith('_Q'):\n",
    "            insert_after = f'ip_{country_prefix}_P85'\n",
    "        elif col_name.endswith('_R_S'):\n",
    "            insert_after = f'ip_{country_prefix}_Q'\n",
    "        else:\n",
    "            insert_after = None\n",
    "\n",
    "        if insert_after in df.columns:\n",
    "            insertion_idx = df.columns.get_loc(insert_after) + 1\n",
    "            df_part1 = df.iloc[:, :insertion_idx]\n",
    "            df_part2 = df.iloc[:, insertion_idx:]\n",
    "            new_col_df = pd.DataFrame(new_columns[col_name])\n",
    "            new_col_df.columns = [col_name]\n",
    "            df = pd.concat([df_part1, new_col_df, df_part2], axis=1)\n",
    "        else:\n",
    "            df[col_name] = new_columns[col_name]\n",
    "\n",
    "    return df\n",
    "\n",
    "def insert_aggregated_rows(df, new_rows, country_prefix):\n",
    "    for row_name, series in new_rows.iterrows():\n",
    "        if row_name.endswith('_N'):\n",
    "            insert_after = f'op_{country_prefix}_M74_M75'\n",
    "        elif row_name.endswith('_Q'):\n",
    "            insert_after = f'op_{country_prefix}_P85'\n",
    "        elif row_name.endswith('_R_S'):\n",
    "            insert_after = f'op_{country_prefix}_Q'\n",
    "        else:\n",
    "            insert_after = None\n",
    "\n",
    "        if insert_after in df.index:\n",
    "            insertion_idx = df.index.get_loc(insert_after) + 1\n",
    "            df_part1 = df.iloc[:insertion_idx, :]\n",
    "            df_part2 = df.iloc[insertion_idx:, :]\n",
    "            new_row_df = pd.DataFrame(series).T\n",
    "            new_row_df.index = [row_name]\n",
    "            df = pd.concat([df_part1, new_row_df, df_part2])\n",
    "        else:\n",
    "            df.loc[row_name] = series\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Function to Add Gross Output Row\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def add_gross_output_row(df):\n",
    "    data_columns = df.filter(regex='^ip_')\n",
    "    gross_output = data_columns.sum(axis=0)\n",
    "    gross_output_row = pd.DataFrame(gross_output).T\n",
    "    gross_output_row.index = ['op_GO']\n",
    "\n",
    "    if len(gross_output_row.columns) != len(df.columns):\n",
    "        raise ValueError(f\"Gross output row has {len(gross_output_row.columns)} values, but the DataFrame has {len(df.columns)} columns.\")\n",
    "\n",
    "    df_with_gross_output = pd.concat([df, gross_output_row], axis=0)\n",
    "    return df_with_gross_output\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Function to Calculate and Insert CPI Weight\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def add_country_cpi_weight(df):\n",
    "    \"\"\"\n",
    "    Calculate the CPI weight (iso_cpi_weight) for each sector based on household consumption.\n",
    "    The new column is added directly to the right of the last consumption-related column for each country.\n",
    "    \"\"\"\n",
    "    # Define the rows to exclude from the household consumption sum\n",
    "    rows_to_exclude = ['op_W2_D21X31', 'op_W2_OP_RES', 'op_W2_OP_NRES', \n",
    "        'op_W2_D1', 'op_W2_D29X39', 'op_W2_B2A3G', 'op_GO']\n",
    "\n",
    "    # Get all country codes from the DataFrame\n",
    "    country_codes = extract_country_codes(df)\n",
    "\n",
    "    for country in country_codes:\n",
    "        # Identify household consumption column for the country\n",
    "        household_col = f'ip_{country}_P3_S14'\n",
    "\n",
    "        # Check if the household consumption column exists\n",
    "        if household_col not in df.columns:\n",
    "            print(f\"Household consumption column {household_col} not found for country {country}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the total household consumption for the country, excluding specified rows\n",
    "        total_household_consumption = df.loc[~df.index.isin(rows_to_exclude), household_col].sum()\n",
    "\n",
    "        # Calculate the country CPI weight as the sector's share of total household consumption\n",
    "        cpi_weight_column = df[household_col] / total_household_consumption\n",
    "\n",
    "        # Determine the insertion index: right after the last related column (P5M)\n",
    "        insert_after = f'ip_{country}_P5M'\n",
    "        if insert_after in df.columns:\n",
    "            insertion_idx = df.columns.get_loc(insert_after) + 1\n",
    "        else:\n",
    "            # If expected column not found, append to the end as a fallback\n",
    "            insertion_idx = len(df.columns)\n",
    "\n",
    "        # Insert the calculated column into the DataFrame at the specific position\n",
    "        df.insert(insertion_idx, f'{country}_cpi_weight', cpi_weight_column)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7. Main Function to Aggregate and Insert Rows/Columns and Add Gross Output\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def aggregate_and_insert_with_gross_output(df):\n",
    "    \"\"\"\n",
    "    Main function to handle the full process of row and column aggregation, insertion, and adding the Gross Output row.\n",
    "    \"\"\"\n",
    "    country_codes = extract_country_codes(df)\n",
    "\n",
    "    for country in country_codes:\n",
    "        # Aggregate and Insert Columns\n",
    "        new_columns = aggregate_columns(df, country)\n",
    "        df = remove_old_columns(df, country)\n",
    "        df = insert_aggregated_columns(df, new_columns, country)\n",
    "\n",
    "        # Aggregate and Insert Rows\n",
    "        new_rows = aggregate_rows(df, country)\n",
    "        df = remove_old_rows(df, country)\n",
    "        df = insert_aggregated_rows(df, new_rows, country)\n",
    "\n",
    "    # Add Gross Output Row at the end\n",
    "    df = add_gross_output_row(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 8. Split Tables into Quadrants\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def extract_tables_manually(df):\n",
    "    num_industries = 56\n",
    "    num_countries = 46\n",
    "    interindustry_size = num_industries * num_countries - 1 # Total rows/columns for interindustry quadrant -1 for index=0\n",
    "\n",
    "    interindustry_row_start = 0\n",
    "    interindustry_row_end = interindustry_size + 1  #+1 for index=0\n",
    "    interindustry_col_start = 1  # Skip the row labels column\n",
    "    interindustry_col_end = interindustry_size + 1  # Columns for industry-to-industry flows\n",
    "\n",
    "    final_demand_row_start = 0\n",
    "    final_demand_row_end = interindustry_size + 1 #+1 for index=0\n",
    "    final_demand_col_start = interindustry_col_end  # Start where the interindustry columns ended\n",
    "    final_demand_col_end = df.shape[1]  # To the end of the data\n",
    "\n",
    "    output_row_start = interindustry_size + 1  #+1 for index=0\n",
    "    output_row_end = interindustry_size + 8  # 7 rows for output data and +1 for index=0\n",
    "    output_col_start = 1  # Same as interindustry flow's start\n",
    "    output_col_end = final_demand_col_start  # Same end as interindustry flow\n",
    "\n",
    "    interindustry_df = df.iloc[interindustry_row_start:interindustry_row_end,\n",
    "                            [0] + list(range(interindustry_col_start, interindustry_col_end))]\n",
    "    \n",
    "    final_demand_df = df.iloc[final_demand_row_start:final_demand_row_end,\n",
    "                            list(range(final_demand_col_start, final_demand_col_end))]\n",
    "\n",
    "    output_df = df.iloc[output_row_start:output_row_end,\n",
    "                            [0] + list(range(output_col_start, output_col_end))]\n",
    "\n",
    "    return interindustry_df, final_demand_df, output_df\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 9. Save the Split Tables\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def save_tables(interindustry_df, final_demand_df, output_df, full_df, year, output_dir):\n",
    "    \"\"\"\n",
    "    Save interindustry, final demand, and output tables, along with the full DataFrame, into respective directories by year.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists for this specific year\n",
    "    year_dir = os.path.join(output_dir, str(year))\n",
    "    os.makedirs(year_dir, exist_ok=True)\n",
    "\n",
    "    # Save each table as a CSV file\n",
    "    interindustry_df.to_csv(os.path.join(year_dir, f'interindustry_table_{year}.csv'), index=True)\n",
    "    final_demand_df.to_csv(os.path.join(year_dir, f'final_demand_table_{year}.csv'), index=True)\n",
    "    output_df.to_csv(os.path.join(year_dir, f'output_table_{year}.csv'), index=True)\n",
    "\n",
    "    # Save the full processed DataFrame with Gross Output row\n",
    "    full_df.to_csv(os.path.join(year_dir, f'full_table_{year}_aggregated_with_GO.csv'), index=True)\n",
    "\n",
    "    print(f\"Processed and saved tables for the year {year} in directory: {year_dir}\")\n",
    " \n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 10. Main Function to Process All Files and Split Tables\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def process_all_files_in_folder(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process all CSV files in the input folder, perform aggregation and insertion,\n",
    "    add the 'Gross Output' row, add CPI weights, split into quadrants, and save.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            print(f\"Processing {filename}...\")\n",
    "\n",
    "            df = pd.read_csv(file_path, index_col=0)\n",
    "            df = apply_sector_mapping(df)\n",
    "            df = prefix_rows(df)\n",
    "            df = prefix_columns(df)\n",
    "\n",
    "            full_df = aggregate_and_insert_with_gross_output(df)\n",
    "            \n",
    "            # Add CPI weights for each sector\n",
    "            full_df = add_country_cpi_weight(full_df)\n",
    "\n",
    "            year = filename.split('_')[0]\n",
    "            interindustry_df, final_demand_df, output_df = extract_tables_manually(full_df)\n",
    "\n",
    "            save_tables(interindustry_df, final_demand_df, output_df, full_df, year, output_dir)\n",
    "\n",
    "# Example usage\n",
    "input_dir = 'C:/Users/danie/Nextcloud/Coding/Masterthesis/data/raw/figaro_tables'\n",
    "output_dir = 'C:/Users/danie/Nextcloud/Coding/Masterthesis/data/processed/processed_tables'\n",
    "\n",
    "# Process all files in the folder\n",
    "process_all_files_in_folder(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f573c7b-04e0-4465-a3b7-c5082c7c7b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
